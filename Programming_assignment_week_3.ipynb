{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment you will be working with `1C` dataset from the final competition. You are asked to encode `item_id` in 4 different ways:\n",
    "\n",
    "    1) Via KFold scheme;  \n",
    "    2) Via Leave-one-out scheme;\n",
    "    3) Via smoothing scheme;\n",
    "    4) Via expanding mean scheme.\n",
    "\n",
    "**You will need to submit** the correlation coefficient between resulting encoding and target variable up to 4 decimal places.\n",
    "\n",
    "### General tips\n",
    "\n",
    "* Fill NANs in the encoding with `0.3343`.\n",
    "* Some encoding schemes depend on sorting order, so in order to avoid confusion, please use the following code snippet to construct the data frame. This snippet also implements mean encoding without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NaN Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NAN_VALUE = 0.3343"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silence NumPy Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python Standard Library\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "# From PyPi\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind this is that we want to convert a categorical value (the item ID) into a numeric one so that we can use non-tree-based methods. But we already have One Hot Encoding, so why do we need this? Well, lets look at how many items we need to encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21,807\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,}\".format(len(sales.item_id.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we're going to have to add nearly 22K columns to the table, which brings up the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) -- adding this many columns means we're going to need a lot more data for the model to work and will significantly increase computation time. Using the Mean Encoding means we'll have to add one more collumn, simplifying our computation and reducing the amount of data needed to fit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the competition task is to make a monthly prediction, we need to aggregate the data to montly level before doing any encodings. The following code-cell serves just that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_by_columns = ['shop_id', 'item_id', 'date_block_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every month, we create a gride from all shops/items combinations for the month. This uses [itertools.product](https://docs.python.org/3/library/itertools.html#itertools.product), which creates the cartesian product of the given collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    block = sales[sales['date_block_num']==block_num]\n",
    "    cur_shops = block['shop_id'].unique()\n",
    "    cur_items = block['item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now turn the grid into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = pd.DataFrame(np.vstack(grid), columns = group_by_columns, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shop_id  item_id  date_block_num\n",
      "0       59    22154               0\n",
      "1       59     2552               0\n",
      "2       59     2554               0\n",
      "3       59     2555               0\n",
      "4       59     2564               0\n"
     ]
    }
   ],
   "source": [
    "print(grid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10913850, 3)\n"
     ]
    }
   ],
   "source": [
    "print(grid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid has all the items sold by each shop for each date-block. The number of rows isn't just shops x items x date-blocks because not every shop is in every date-block and not every shop sold every item (or even the same items every block).\n",
    "\n",
    "Now we will use [groupby](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) to group the data by shop_id, item_id and month (date_block_num) and then get the aggregated summed values for the item count per day (we're going to sum up the items sold per day to get a value for the month) and [rename](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) the summed item count column to target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = sales.groupby(group_by_columns, as_index = False)\n",
    "grouped = grouped[\"item_cnt_day\"].sum()\n",
    "grouped = grouped.rename(columns = {\"item_cnt_day\": \"target\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shop_id  item_id  date_block_num  target\n",
      "0        0       30               1    31.0\n",
      "1        0       31               1    11.0\n",
      "2        0       32               0     6.0\n",
      "3        0       32               1    10.0\n",
      "4        0       33               0     3.0\n"
     ]
    }
   ],
   "source": [
    "print(grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Now join the aggregated data to the grid (with [merge](https://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.merge(grid, grouped, how = 'left', on = group_by_columns).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shop_id  item_id  date_block_num  target\n",
      "0       59    22154               0     1.0\n",
      "1       59     2552               0     0.0\n",
      "2       59     2554               0     0.0\n",
      "3       59     2555               0     0.0\n",
      "4       59     2564               0     0.0\n"
     ]
    }
   ],
   "source": [
    "print(all_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Sort the data by the month, shop, and item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data.sort_values(['date_block_num','shop_id','item_id'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        shop_id  item_id  date_block_num  target\n",
      "139255        0       19               0     0.0\n",
      "141495        0       27               0     0.0\n",
      "144968        0       28               0     0.0\n",
      "142661        0       29               0     0.0\n",
      "138947        0       32               0     6.0\n"
     ]
    }
   ],
   "source": [
    "print(all_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "When we compare all_data to grouped, the difference might not be so obvious, they have the same columns and look pretty similar, but if you look at the all_data.target column you can see that there's a lot of os. That's because grouped only has the cases where there were sales, but all_data had cases where there weren't any sales for particular (shop_id, item_id, date_block_num) combination, so it filled in the 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,304,726\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,}\".format(all_data.shape[0] - grouped.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all_data had over 9M more rows than grouped did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [shop_id, item_id, date_block_num, target]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(grouped[(grouped.shop_id == 0) & (grouped.item_id == 19) & (grouped.date_block_num == 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And grouped didn't have any entry for the first item in the previous `all_data` head-table, which is why the target value is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Encodings Without Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done the technical work, we are ready to actually *mean encode* the desired `item_id` variable. \n",
    "\n",
    "Here are two ways to implement mean encoding features *without* any regularization. You can use this code as a starting point to implement regularized techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Calculate a Mapping: {item_id:target_mean}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: we're going to calculate the mean count for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_id_target_mean = all_data.groupby('item_id').target.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our non-regularized case, we just [map]() the computed means to the `item_id`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['item_target_enc'] = all_data['item_id'].map(item_id_target_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we are mapping a series (`item_id_target_mean`) to a column `item_id` in a data frame `all_data`. Wherever an item in the `item_id` column matches the index of our `item_id_target_mean` series it will replace the item with the value in the `item_id_target_mean` that matches the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example: let's look at the hea of the `item_id_target_mean` series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target_mean': item_id\n",
      "0    0.020000\n",
      "1    0.023810\n",
      "2    0.019802\n",
      "3    0.019802\n",
      "4    0.020000\n",
      "Name: target, dtype: float64}\n"
     ]
    }
   ],
   "source": [
    "print(dict(target_mean = item_id_target_mean.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "So, let's look at index 1 -- its value is -.02381, so this means we would expect that all the items with ID1 would also have this value in the `item_target_end` column. Let's double-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         shop_id  item_id  date_block_num  target  item_target_enc\n",
      "5664585        2        1              15     0.0          0.02381\n",
      "5651037        3        1              15     0.0          0.02381\n",
      "5657811        4        1              15     0.0          0.02381\n",
      "5671359        5        1              15     0.0          0.02381\n",
      "5623941        6        1              15     0.0          0.02381\n"
     ]
    }
   ],
   "source": [
    "print(all_data[all_data.item_id==1].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "It looks right. Let's make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert all(all_data[all_data.item_id==1] == 0.0238095)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this wasn't exhaustive, but at least that one item checks out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're given the value to fill in for the missing entries (0.3343) without explanation. I don't really know hwere it comes from. It's around, but not exactly the 84-percentile, but, anyway, let's use it (actually, if you chick it there aren't any NaN value, curious)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(all_data.item_target_enc.hasnans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this next line doens't seem to do anything, but is part of the given code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['item_target_enc'].fillna(NAN_VALUE, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate the [Person Correlation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html) between our calculated mean and the target values. This [value](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) ranges from -1 to 1 and represents how much of a linear correlation there is between two variables: -1 means they're completely negatively correlative and +1 means they're completely positively correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.483038698862\n"
     ]
    }
   ],
   "source": [
    "encoded_feature = all_data['item_target_enc'].values\n",
    "first_correlation = np.corrcoef(all_data['target'].values, encoded_feature)[0][1]\n",
    "print(first_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Since our value is between 0 and 1 it does describe the target to some degree, albeit not perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the `.target.mean()` function, `transform` will return a dataframe with an index like in `all_data`. Basically, this single line of code is equivalent to the first lines from the Method 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['item_target_enc'] = all_data.groupby('item_id')['target'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['item_target_enc'].fillna(NAN_VALUE, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.483038698862\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "encoded_feature = all_data['item_target_enc'].values\n",
    "second_correlation = np.corrcoef(all_data['target'].values, encoded_feature)[0][1]\n",
    "print(second_correlation)\n",
    "print(abs(first_correlation - second_correlation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the printed value? It is the correlation coefficient between the target variable and your new encoded feature. You need to **compute the correlation coefficient between the encodings** that you will implement and **submit those to coursera**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. KFold scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained starting at 41 sec of [Regularization video](https://www.coursera.org/learn/competitive-data-science/lecture/LGYQ2/regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implement KFold scheme with five folds. Use KFold(5) from sklearn.model_selection. \n",
    "\n",
    "1. Split your data in 5 folds with [`sklearn.model_selection.KFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) with `shuffle=False` argument.\n",
    "2. Iterate through folds: use all but the current fold to calculate mean target for each level `item_id`, and  fill the current fold.\n",
    "\n",
    "See the **Method 1** from the example implementation. In particular learn what `map` and [pd.Series.map](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html) functions do. They are pretty handy in many situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "kf = model_selection.KFold(5, shuffle=False)\n",
    "all_data['item_target_enc'] = np.nan\n",
    "\n",
    "for tr_ind, val_ind in kf.split(all_data):\n",
    "    X_tr, X_val = all_data.iloc[tr_ind], all_data.iloc[val_ind]\n",
    "    all_data.loc[all_data.index[val_ind], 'item_target_enc'] = X_val['item_id'].map(X_tr.groupby('item_id').target.mean())\n",
    "    \n",
    "all_data['item_target_enc'].fillna(0.3343, inplace=True)\n",
    "encoded_feature = all_data['item_target_enc'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "You will need to compute correlation like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41645907128\n",
      "Current answer for task KFold_scheme is: 0.41645907128\n"
     ]
    }
   ],
   "source": [
    "corr = np.corrcoef(all_data['target'].values, encoded_feature)[0][1]\n",
    "print(corr)\n",
    "grader.submit_tag('KFold_scheme', corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#folder = KFold(n_splits=5, shuffle=False)\n",
    "#column = \"item_id\"\n",
    "#encoded_column = column + \"_mean_target\"\n",
    "#train_new = pd.DataFrame(index=all_data.index, columns=all_data.columns)\n",
    "#train_new[encoded_column] = np.nan\n",
    "#for training_index, validation_index in folder.split(all_data):\n",
    " #   x_train = all_data.iloc[training_index].copy()\n",
    "  #  x_validation = all_data.iloc[validation_index].copy()\n",
    "   # means = x_validation[column].map(x_train.groupby(column).target.mean())\n",
    "    #x_validation[encoded_column] = means\n",
    "    # train_new is a dataframe copy we made of the training data\n",
    "    #train_new.iloc[validation_index] = x_validation\n",
    "#train_new.fillna(NAN_VALUE, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoded_feature = train_new.item_id_mean_target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Leave-one-out scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, implement leave-one-out scheme. \n",
    "Note: if you just simply set the number of folds to the number of samples and run the code from the **KFold scheme**, you will probably wait for a very long time. \n",
    "\n",
    "To implement a faster version, note, that to calculate mean target value using all the objects but one **given object**, you can:\n",
    "\n",
    "1. Calculate sum of the target values using all the objects.\n",
    "2. Then subtract the target of the *given object* and divide the resulting value by `n_objects - 1`. \n",
    "\n",
    "Note that you do not need to perform `1.` for every object. And `2.` can be implemented without any `for` loop.\n",
    "\n",
    "It is the most convenient to use `.transform` function as in **Method 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we calculate the sum and count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_out_sum = all_data['item_id'].map(all_data.groupby('item_id').target.sum())\n",
    "leave_one_out_count = all_data['item_id'].map(all_data.groupby('item_id').target.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        shop_id  item_id  date_block_num  target  item_target_enc\n",
      "139255        0       19               0     0.0         0.022727\n",
      "141495        0       27               0     0.0         0.056911\n",
      "144968        0       28               0     0.0         0.141414\n",
      "142661        0       29               0     0.0         0.037500\n",
      "138947        0       32               0     6.0         1.316088\n"
     ]
    }
   ],
   "source": [
    "print(all_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Now, we'll calculate how many items there are once you leave one out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['item_target_enc'] = ((leave_one_out_sum - all_data['target']))/(leave_one_out_count-1)\n",
    "all_data['item_target_enc'].fillna(0.3343, inplace=True)                                                \n",
    "encoded_feature = all_data['item_target_enc'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.480384831129\n",
      "Current answer for task Leave-one-out_scheme is: 0.480384831129\n"
     ]
    }
   ],
   "source": [
    "corr = np.corrcoef(all_data['target'].values, encoded_feature)[0][1]\n",
    "print(corr)\n",
    "grader.submit_tag('Leave-one-out_scheme', corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explained starting at 4:03 of [Regularization video](https://www.coursera.org/learn/competitive-data-science/lecture/LGYQ2/regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement smoothing scheme with $\\alpha = 100$. Use the formula from the first slide in the video and $0.3343$ as `globalmean`. Note that `nrows` is the number of objects that belong to a certain category (not the number of rows in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 100\n",
    "globalmean = 0.3343\n",
    "train_new = all_data.copy()\n",
    "nrows = train_new.groupby('item_id').size()\n",
    "means = train_new.groupby('item_id').target.agg('mean')\n",
    "\n",
    "score = (np.multiply(means,nrows)  + globalmean*alpha) / (nrows+alpha)\n",
    "train_new['smooth'] = train_new['item_id']\n",
    "train_new['smooth'] = train_new['smooth'].map(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48181987971\n",
      "Current answer for task Smoothing_scheme is: 0.48181987971\n"
     ]
    }
   ],
   "source": [
    "encoded_feature = train_new['smooth'].values\n",
    "corr = np.corrcoef(all_data['target'].values, encoded_feature)[0][1]\n",
    "print(corr)\n",
    "grader.submit_tag('Smoothing_scheme', corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Expanding Mean Scheme\n",
    "\n",
    "Explained starting at 5:50 of [Regularization video](https://www.coursera.org/learn/competitive-data-science/lecture/LGYQ2/regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, implement the *expanding mean* scheme. It is basically already implemented for you in the video, but you can challenge yourself and try to implement it yourself. You will need [`cumsum`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.cumsum.html) and [`cumcount`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.cumcount.html) functions from pandas. Where `cumsum` is the cumulative sum for each group and `cumcount` is the number of each item in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2307657546.0\n"
     ]
    }
   ],
   "source": [
    "cumsum = all_data.groupby('item_id').target.cumsum() - all_data['target']\n",
    "cumcnt = all_data.groupby('item_id').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new[\"mean_target\"] = cumsum/cumcnt\n",
    "train_new['mean_target'].fillna(0.3343, inplace=True)\n",
    "encoded_feature = train_new['mean_target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.502524521108\n",
      "Current answer for task Expanding_mean_scheme is: 0.502524521108\n"
     ]
    }
   ],
   "source": [
    "corr = np.corrcoef(all_data['target'].values, encoded_feature)[0][1]\n",
    "print(corr)\n",
    "grader.submit_tag('Expanding_mean_scheme', corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. Note: Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these numbers:\n",
      "Task KFold_scheme: 0.41645907128\n",
      "Task Leave-one-out_scheme: 0.480384831129\n",
      "Task Smoothing_scheme: 0.48181987971\n",
      "Task Expanding_mean_scheme: 0.502524521108\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'valexanderma@gmail.com'\n",
    "STUDENT_TOKEN = 'sVEpzsGDo17fE7I3'\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
